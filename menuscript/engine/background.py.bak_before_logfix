#!/usr/bin/env python3
"""
menuscript.engine.background â€” safe, simple job queue + worker (file-backed)

Design notes:
 - Small, robust JSON-backed job store (data/jobs/jobs.json)
 - Logs to data/logs/<job_id>.log
 - Plugin-first execution: attempt to call plugin.run(target, args, label)
 - Fallback to subprocess.run([tool, ...]) if plugin not available
 - Worker supports foreground (--fg) and background start
 - Long-running tool kill timeout: 300s (5 minutes)
 - Minimal, clean logging to worker.log and per-job logs
"""

from __future__ import annotations
import os
import sys
import json
import time
import tempfile
import shutil
import subprocess
import threading
from typing import List, Dict, Optional, Any

ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
DATA_DIR = os.path.join(ROOT, "data")
JOBS_DIR = os.path.join(DATA_DIR, "jobs")
LOGS_DIR = os.path.join(DATA_DIR, "logs")
JOBS_FILE = os.path.join(JOBS_DIR, "jobs.json")
WORKER_LOG = os.path.join(LOGS_DIR, "worker.log")
JOB_TIMEOUT_SECONDS = 300  # 5 minutes - H2 selected

_lock = threading.Lock()

def _ensure_dirs():
    os.makedirs(JOBS_DIR, exist_ok=True)
    os.makedirs(LOGS_DIR, exist_ok=True)

def _read_jobs() -> List[Dict[str,Any]]:
    _ensure_dirs()
    if not os.path.exists(JOBS_FILE):
        return []
    try:
        with open(JOBS_FILE, "r", encoding="utf-8") as fh:
            return json.load(fh)
    except Exception:
        # if corrupt, move aside and start fresh
        try:
            corrupt = JOBS_FILE + ".corrupt." + str(int(time.time()))
            shutil.move(JOBS_FILE, corrupt)
            _append_worker_log(f"jobs file corrupt; moved to {corrupt}")
        except Exception:
            pass
        return []

def _write_jobs(jobs: List[Dict[str,Any]]):
    _ensure_dirs()
    tmp = tempfile.NamedTemporaryFile("w", delete=False, dir=JOBS_DIR, encoding="utf-8")
    try:
        json.dump(jobs, tmp, indent=2, ensure_ascii=False)
        tmp.flush()
        os.fsync(tmp.fileno())
        tmp.close()
        os.replace(tmp.name, JOBS_FILE)
    finally:
        if os.path.exists(tmp.name):
            try:
                os.remove(tmp.name)
            except Exception:
                pass

def _append_worker_log(msg: str):
    _ensure_dirs()
    ts = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
    line = f"{ts} {msg}\n"
    with open(WORKER_LOG, "a", encoding="utf-8", errors="replace") as fh:
        fh.write(line)

def _next_job_id(jobs: List[Dict[str,Any]]) -> int:
    maxid = 0
    for j in jobs:
        try:
            if isinstance(j.get("id"), int) and j["id"] > maxid:
                maxid = j["id"]
        except Exception:
            continue
    return maxid + 1

# Public API: enqueue, list, get
def enqueue_job(tool: str, target: str, args: List[str], label: str="") -> int:
    with _lock:
        jobs = _read_jobs()
        jid = _next_job_id(jobs)
        now = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        job = {
            "id": jid,
            "tool": tool,
            "target": target,
            "args": args or [],
            "label": label or "",
            "status": "queued",
            "created_at": now,
            "started_at": None,
            "finished_at": None,
            "result_scan_id": None,
            "error": None,
            "log": os.path.join(JOBS_DIR, f"{jid}.log")
        }
        jobs.append(job)
        _write_jobs(jobs)
    _append_worker_log(f"enqueued {jid} {tool} {target}")
    return jid

def list_jobs(limit:int=100) -> List[Dict[str,Any]]:
    jobs = _read_jobs()
    # newest first
    return sorted(jobs, key=lambda x: x.get("created_at",""), reverse=True)[:limit]

def get_job(jid:int) -> Optional[Dict[str,Any]]:
    jobs = _read_jobs()
    for j in jobs:
        if j.get("id") == jid:
            return j
    return None

def _update_job(jid:int, **fields):
    with _lock:
        jobs = _read_jobs()
        changed = False
        for j in jobs:
            if j.get("id") == jid:
                j.update(fields)
                changed = True
                break
        if changed:
            _write_jobs(jobs)

# Plugin invocation helper
def _try_run_plugin(tool: str, target: str, args: List[str], label: str, log_path: str) -> bool:
    """
    Attempt to call a plugin's run() signature:
      plugin.run(target, args, label, logpath)
    Return True if the plugin was found and executed (regardless of success).
    """
    try:
        import importlib
        plugins_pkg = importlib.import_module("menuscript.engine.loader")
        discover = getattr(plugins_pkg, "discover_plugins", None)
        if not discover:
            return False
        plugins = discover()
        p = plugins.get(tool) or None
        if not p:
            # try to match by name substring
            for v in plugins.values():
                try:
                    if (getattr(v,"tool","") or "").lower() == tool.lower() or (getattr(v,"name","") or "").lower().find(tool.lower()) != -1:
                        p = v
                        break
                except Exception:
                    continue
        if not p:
            return False
        runfn = getattr(p, "run", None)
        if callable(runfn):
            # plugin may write its own logs; but we also capture exceptions
            try:
                runfn(target, args, label, log_path)
                return True
            except Exception as e:
                with open(log_path, "a", encoding="utf-8", errors="replace") as fh:
                    fh.write(f"PLUGIN ERROR: {e}\n")
                return True
        return False
    except Exception:
        return False

def _run_subprocess(tool: str, target: str, args: List[str], log_path: str, timeout: int = JOB_TIMEOUT_SECONDS) -> int:
    cmd = [tool] + (args or [])
    # Replace <target> placeholder if present
    cmd = [c.replace("<target>", target) for c in cmd]
    with open(log_path, "a", encoding="utf-8", errors="replace") as fh:
        fh.write(f"--- Running: {' '.join(cmd)} ---\n")
        fh.flush()
        try:
            proc = subprocess.run(cmd, stdout=fh, stderr=subprocess.STDOUT, timeout=timeout, check=False)
            return proc.returncode
        except subprocess.TimeoutExpired:
            fh.write(f"ERROR: command timed out after {timeout} seconds\n")
            return 124
        except FileNotFoundError:
            fh.write(f"ERROR: tool not found: {cmd[0]}\n")
            return 127
        except Exception as e:
            fh.write(f"ERROR: exception: {e}\n")
            return 1

def run_job(jid:int) -> None:
    """
    Run a single job by id. Updates job status fields in-place.
    Plugin-first: tries plugin.run(); if plugin not executed, runs subprocess.
    """
    job = get_job(jid)
    if not job:
        _append_worker_log(f"run_job: job {jid} not found")
        return
    log_path = job.get("log") or os.path.join(JOBS_DIR, f"{jid}.log")
    # ensure the job log exists and warn in worker log
    _ensure_dirs()
    if not os.path.exists(os.path.dirname(log_path)):
        os.makedirs(os.path.dirname(log_path), exist_ok=True)

    _update_job(jid, status="running", started_at=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()))
    _append_worker_log(f"job {jid} running")

    try:
        # Try plugin first
        plugin_executed = _try_run_plugin(job.get("tool",""), job.get("target",""), job.get("args",[]), job.get("label",""), log_path)
        rc = 0
        if not plugin_executed:
            # fallback: call the tool name (nmap/gobuster/nikto/etc.)
            rc = _run_subprocess(job.get("tool",""), job.get("target",""), job.get("args",[]), log_path, timeout=JOB_TIMEOUT_SECONDS)
        # mark finished
        now = time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())
        status = "done" if rc == 0 else "error"
        _update_job(jid, status=status, finished_at=now)
        _append_worker_log(f"job {jid} finished status={status} rc={rc}")
    except Exception as e:
        _update_job(jid, status="error", error=str(e), finished_at=time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime()))
        _append_worker_log(f"job {jid} crashed: {e}")

# Worker loop
def worker_loop(poll_interval: float = 2.0):
    """
    Simple worker loop:
      - polls jobs JSON for next queued job
      - runs it synchronously (run_job)
      - sleeps when none found
    Use ctrl-C to stop in foreground.
    """
    _ensure_dirs()
    _append_worker_log("menuscript background worker: starting loop")
    try:
        while True:
            jobs = _read_jobs()
            queued = [j for j in jobs if j.get("status") == "queued"]
            if not queued:
                time.sleep(poll_interval)
                continue
            # pick earliest queued (by created_at)
            queued_sorted = sorted(queued, key=lambda x: x.get("created_at",""))
            job = queued_sorted[0]
            jid = job.get("id")
            try:
                run_job(jid)
            except Exception as e:
                _append_worker_log(f"run_job exception for {jid}: {e}")
            # loop again immediately
    except KeyboardInterrupt:
        _append_worker_log("worker: KeyboardInterrupt, shutting down")
    except Exception as e:
        _append_worker_log(f"worker loop stopped with exception: {e}")

# start_worker: convenience wrapper (detach support)
def start_worker(detach: bool = True, fg: bool = False):
    """
    If detach True, spawn a background process that runs this module's worker_loop().
    If fg True, run in current process (blocking).
    """
    if fg:
        worker_loop()
        return
    if detach:
        python = sys.executable or "python3"
        cmd = [python, "-u", "-c", "import sys; from menuscript.engine.background import worker_loop; worker_loop()"]
        # Launch detached
        subprocess.Popen(cmd, stdout=open(WORKER_LOG,"a"), stderr=subprocess.STDOUT, close_fds=True)
        _append_worker_log("Started background worker (detached)")
